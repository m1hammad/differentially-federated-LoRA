# Differentially-Federated-LoRA

A privacy-preserving federated learning framework that integrates Lowâ€‘Rank Adaptation (LoRA) with Differential Privacy (DP) on transformer models using Flower and Opacus.

---

## ğŸš€ Features

* **LoRA Fineâ€‘Tuning**: Efficiently adapt large transformer models with lowâ€‘rank adapters.
* **Federated Learning**: Distribute training across multiple clients without sharing raw data (FedAvg).
* **Differential Privacy**: Guarantee \$(arepsilon, \delta)\$â€‘DP by adding Gaussian noise and clipping gradients.
* **Modular Design**: Plug-and-play modules for device management, data loading, model adaptation, privacy, and federated orchestration.
* **Endâ€‘toâ€‘End Pipeline**: Single notebook (`main.ipynb`) to run experiments, collect metrics, and visualize results.

---

## ğŸ“‹ Prerequisites

* PythonÂ 3.8+
* PyTorch 1.\*\*
* Flower 1.\*\*
* Opacus 1.\*\*
* Hugging Face Transformers & Datasets
* peft 0.\*\*
* scikit-learn
* matplotlib, seaborn

Install dependencies via:

```bash
pip install -r requirements.txt
```

---

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ clients.py              # Flower federated client implementation
â”œâ”€â”€ server.py               # Flower federated server setup
â”œâ”€â”€ lora.py                 # LoRA adapter integration on transformers
â”œâ”€â”€ differential_privacy.py # Opacus DP training wrapper
â”œâ”€â”€ data.py                 # IMDB dataset loading & tokenization
â”œâ”€â”€ device.py               # GPU/CPU/MPS device utilities
â”œâ”€â”€ multiprocessing_helpers.py # Helpers for parallel client processes
â”œâ”€â”€ main.ipynb              # Orchestrates experiments & visualizations
â”œâ”€â”€ results_2.csv           # Logged metrics from experimental runs
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project overview (this file)
```

---

## ğŸ’¡ Quick Start

### 1. Launch the Federated Server

```bash
flower-superlink --insecure --serverappio-api-address="0.0.0.0:9090"
```

### 2. Launch One or More Clients

Change the client API port per client:

```bash
flower-supernode --insecure --superlink="localhost:9092" --clientappio-api-address="0.0.0.0:9096"
# For additional clients, increment the 9096 port accordingly
```

### 3. Run Experiments in `main.ipynb`

* Define scenarios:

  * **LoRA Only**
  * **LoRA + Federated Learning**
  * **LoRA + Federated Learning + Differential Privacy**
* Select transformer models and LoRA target modules.
* Execute cells to start server & spawn clients (or run local training).
* Collect metrics and generate plots (FiguresÂ 1â€“15 as in the report).

---

## ğŸ” Module Overview

### `device.py`

* Detects available hardware (CUDA, MPS, CPU) and moves models/tensors accordingly.

### `data.py`

* Loads IMDB dataset, tokenizes with Hugging Face tokenizer (padding/truncation), and returns PyTorch DataLoaders.

### `lora.py`

* Creates a `peft.LoraConfig` and wraps a transformer model to fineâ€‘tune only lowâ€‘rank adapters.

### `differential_privacy.py`

* Wraps model and optimizer in `opacus.PrivacyEngine` with noise multiplier and gradient clipping.

### `clients.py`

* Implements `FlowerClient` for federated training and evaluation:

  * `get_parameters`, `set_parameters` for weight exchange
  * `fit` trains locally (with optional DP)
  * `evaluate` returns loss & accuracy metrics

### `server.py`

* Configures and starts Flower server using `FedAvg` strategy and orchestrates client rounds.

### `main.ipynb`

* Orchestrates full pipeline:

  1. Define scenarios & models
  2. Launch server process
  3. Start client processes or local training
  4. Log metrics to CSV
  5. Plot accuracy, precision, recall, F1, ROCâ€‘AUC, loss, and training time.

---

## ğŸ“Š Results & Metrics

Supported metrics (logged in `results_2.csv` and visualized):

* **Accuracy, Precision, Recall, F1 Score**
* **ROCâ€‘AUC**
* **Loss**
* **Training Time**
* **Communication Overhead** (implicit via Flower logs)

Refer to the reportâ€™s FiguresÂ 1â€“15 for detailed comparisons across scenarios and models.

---

## ğŸš§ Future Work

* **Privacyâ€‘Accuracy Tradeâ€‘off**: Vary \$\varepsilon \in {0.1,1,5,10}\$ and clipping norms.
* **Alternative FL Strategies**: Implement FedProx, FedAdam, FedNova.
* **Personalized FL**: Explore PerFedAvg and clientâ€specific models.
* **Security Testing**: Evaluate robustness against membership inference and poisoning attacks.
* **Gradient & Client Contribution Visualization**: Extend notebook to visualize update magnitudes.

---

## ğŸ“„ License

This project is provided under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Contributions

Contributions, issues, and feature requests are welcome! Please open an issue or submit a pull request.
